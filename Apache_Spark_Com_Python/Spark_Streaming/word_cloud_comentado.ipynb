{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O NLTK é um kit de ferramentas para trabalharmos com linguagem natural no Python\n",
    "# Vamos utilizar apenas as definições de Stop Words do NLTK para removê-las da nossa word cloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython import display\n",
    "import time\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"WordCloud\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_tweets(df):\n",
    "    words = df \\\n",
    "        .select(f.explode(f.split(f.lower('_c0'), \" \")) \\                     # Adicionei a função lower para colocar todo o texto em minúsculo O \"_c0\" é o nome da coluna do DataFrame que contém os dados\n",
    "        .alias(\"word\")) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\       # Retira os endereço web. O HTTP está em maiúsculo por conta da função upper utilizada acima\n",
    "        .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\          # Remove os nomes dos usuário do Twitter (@nome_usuário)\n",
    "        .withColumn('word', f.regexp_replace('word', 'rt', '')) \\             # Remove a marcação RT dos retweets\n",
    "        .na.replace('', None) \\                                               # Transforma vazios em None\n",
    "        .na.drop()                                                            # Retira os valor nulos\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trata_tweets(df):\n",
    "    words = df \\\n",
    "        .select(f.explode(f.split(f.lower('_c0'), \" \")) \\\n",
    "        .alias(\"word\")) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'http\\S+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', r'@\\w+', '')) \\\n",
    "        .withColumn('word', f.regexp_replace('word', 'rt', '')) \\\n",
    "        .na.replace('', None) \\\n",
    "        .na.drop()\n",
    "    return words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1] O método collect do Spark é uma action e neste ponto é bom dar destaque ao conceito de Lazy Evaluation do Spark. Em resumo, no Spark temos funções de transformação e ação. Quando utilizamos as transformations o Spark cria um plano de ação e não executa a tarefa imediatamente. Este plano de ação é executado apenas quando utilizamos uma action. Isso permite que o Spark gere um plano de execução otimizado, mesclando algumas transformações e até mesmo pulando algumas que sejam desnecessárias.\n",
    "\n",
    "[2] Documentação do wordcloud: https://amueller.github.io/word_cloud/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stopwords.words('portuguese')                               # Criando uma variável com as Stop Word em português. Se for utilizar tweets em inglês basta modificar 'portuguese' para 'english'\n",
    "stops.append('futebol')                                             # Pode ser interessante retirar a palavra utilizada na pesquisa\n",
    "plt.figure(figsize=(20, 10))                                        # Cria a figura e defini o tamanho dela (largura, altura)\n",
    "\n",
    "while True:\n",
    "    try:      # Este try/except foi colocado para tratar os erros que aparecem quando interrompemos o processo\n",
    "        words = spark.read.csv('./csv', encoding='latin1')          # Lendo o conjunto de arquivos CSV na pasta /csv\n",
    "        words = trata_tweets(words)                                 # Aplicando nossa função de tratamento\n",
    "        rows = words.collect()                                      # Transformando o DataFrame em uma lista de linhas [1]\n",
    "        all_words = ''\n",
    "        for row in rows:\n",
    "            all_words = all_words + ' ' + row['word']\n",
    "\n",
    "        wordcloud = WordCloud(stopwords=stops,\n",
    "                              background_color=\"black\",\n",
    "                              width=1920,\n",
    "                              height=1080,\n",
    "                              max_words=100\n",
    "                              ).generate(all_words)                 # Word cloud simples. Mais detalhes em [2]\n",
    "\n",
    "        plt.cla()                                                   # Limpa os eixos do gráfico\n",
    "        plt.axis('off')                                             # Oculta as marcações dos eixos\n",
    "        plt.imshow(wordcloud)                                       # Utilizado para exibir os dados como uma imagem\n",
    "        display.display(plt.gcf())                                  # Mostrando a nossa word cloud no output do notebook\n",
    "        display.clear_output(wait=True)                             # Limpa o output do notebook\n",
    "        time.sleep(5)\n",
    "    except KeyboardInterrupt:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit (windows store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "541f1ccb820e06d22b0e0ca226fd1b36bbd00a40fedd51f8e91c1934c9becc0a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
